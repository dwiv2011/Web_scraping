{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deepak Dwivedi\n",
    "Question 3: News_Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Neon\\\\Desktop\\\\working\\\\Incampus 1\\\\Data Collection\\\\Project'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\Neon\\\\Desktop\\\\working\\\\Incampus 1\\\\Data Collection\\\\Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the request\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=\"cfa29cfcb3954ea4871e80c6299454b0\"\n",
    "\n",
    "Url=\"https://newsapi.org/v2/top-headlines?sources=techcrunch&apiKey=\"+key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading url as json\n",
    "page = requests.get(Url).json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting value of key \"article\"\n",
    "art=page[\"articles\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating empty list to append data\n",
    "\n",
    "title = []\n",
    "auth=[]\n",
    "desc=[]\n",
    "cont=[]\n",
    "idr = []\n",
    "name=[]\n",
    "for dd in art:\n",
    "    idr.append(dd[\"source\"][\"id\"])## accessing sub dictionary\n",
    "    name.append(dd[\"source\"][\"name\"])## accessing sub dictionary\n",
    "    title.append(dd['title'])# fetching title from each dictionary \"dd\"\n",
    "    auth.append(dd['author'])## author name \n",
    "    desc.append(dd['description'])## Description Name\n",
    "    cont.append(dd['content'])# Content \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Data Frame \n",
    "\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(zip(idr,name,title,auth,desc,cont),columns=[\"SOurce ID \", \"Source Name\",\"Title\",\"Author\",\"Description\",\"Content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOurce ID</th>\n",
       "      <th>Source Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Description</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Jackson Square Ventures just closed its third ...</td>\n",
       "      <td>Connie Loizos</td>\n",
       "      <td>Jackson Square Ventures (JSV), an eight-year-o...</td>\n",
       "      <td>Jackson Square Ventures (JSV), an eight-year-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Announcing TechCrunch Robotics &amp; AI on March 3...</td>\n",
       "      <td>Ned Desmond</td>\n",
       "      <td>Robotics is back! We are excited to announce t...</td>\n",
       "      <td>Robotics is back! We are excited to announce t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Uber lays off another ~350 across Eats, self-d...</td>\n",
       "      <td>Megan Rose Dickey</td>\n",
       "      <td>Uber has just laid off around 350 employees ac...</td>\n",
       "      <td>Uber has just laid off around 350 employees ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Founder’s guide to the pre-IPO secondary market</td>\n",
       "      <td>Ryan Conner</td>\n",
       "      <td>The increase in activity in the pre-IPO second...</td>\n",
       "      <td>The increase in activity in the pre-IPO second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Daily Crunch: Facebook has a weak stance on po...</td>\n",
       "      <td>Anthony Ha</td>\n",
       "      <td>The Daily Crunch is TechCrunch’s roundup of ou...</td>\n",
       "      <td>The Daily Crunch is TechCrunch’s roundup of ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Shipping giant Pitney Bowes hit by ransomware</td>\n",
       "      <td>Zack Whittaker</td>\n",
       "      <td>Shipping tech giant Pitney Bowes has confirmed...</td>\n",
       "      <td>Shipping tech giant Pitney Bowes has confirmed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Disney+ tweets all the movies and shows coming...</td>\n",
       "      <td>Sarah Perez</td>\n",
       "      <td>In an impressive bit of pre-launch marketing, ...</td>\n",
       "      <td>In an impressive bit of pre-launch marketing, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Google updates its Titan security keys with USB-C</td>\n",
       "      <td>Zack Whittaker</td>\n",
       "      <td>Google has revealed its latest Titan security ...</td>\n",
       "      <td>Google has revealed its latest Titan security ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Fortnite's black hole stunt is the kind of alp...</td>\n",
       "      <td>Jordan Crook</td>\n",
       "      <td>As you are likely already aware, Epic Games is...</td>\n",
       "      <td>As you are likely already aware, Epic Games is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>India's Reliance Jio unveils video call assist...</td>\n",
       "      <td>Manish Singh</td>\n",
       "      <td>Before Google moves to bring its human-soundin...</td>\n",
       "      <td>Before Google moves to bring its human-soundin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SOurce ID  Source Name                                              Title  \\\n",
       "0  techcrunch  TechCrunch  Jackson Square Ventures just closed its third ...   \n",
       "1  techcrunch  TechCrunch  Announcing TechCrunch Robotics & AI on March 3...   \n",
       "2  techcrunch  TechCrunch  Uber lays off another ~350 across Eats, self-d...   \n",
       "3  techcrunch  TechCrunch    Founder’s guide to the pre-IPO secondary market   \n",
       "4  techcrunch  TechCrunch  Daily Crunch: Facebook has a weak stance on po...   \n",
       "5  techcrunch  TechCrunch      Shipping giant Pitney Bowes hit by ransomware   \n",
       "6  techcrunch  TechCrunch  Disney+ tweets all the movies and shows coming...   \n",
       "7  techcrunch  TechCrunch  Google updates its Titan security keys with USB-C   \n",
       "8  techcrunch  TechCrunch  Fortnite's black hole stunt is the kind of alp...   \n",
       "9  techcrunch  TechCrunch  India's Reliance Jio unveils video call assist...   \n",
       "\n",
       "              Author                                        Description  \\\n",
       "0      Connie Loizos  Jackson Square Ventures (JSV), an eight-year-o...   \n",
       "1        Ned Desmond  Robotics is back! We are excited to announce t...   \n",
       "2  Megan Rose Dickey  Uber has just laid off around 350 employees ac...   \n",
       "3        Ryan Conner  The increase in activity in the pre-IPO second...   \n",
       "4         Anthony Ha  The Daily Crunch is TechCrunch’s roundup of ou...   \n",
       "5     Zack Whittaker  Shipping tech giant Pitney Bowes has confirmed...   \n",
       "6        Sarah Perez  In an impressive bit of pre-launch marketing, ...   \n",
       "7     Zack Whittaker  Google has revealed its latest Titan security ...   \n",
       "8       Jordan Crook  As you are likely already aware, Epic Games is...   \n",
       "9       Manish Singh  Before Google moves to bring its human-soundin...   \n",
       "\n",
       "                                             Content  \n",
       "0  Jackson Square Ventures (JSV), an eight-year-o...  \n",
       "1  Robotics is back! We are excited to announce t...  \n",
       "2  Uber has just laid off around 350 employees ac...  \n",
       "3  The increase in activity in the pre-IPO second...  \n",
       "4  The Daily Crunch is TechCrunch’s roundup of ou...  \n",
       "5  Shipping tech giant Pitney Bowes has confirmed...  \n",
       "6  In an impressive bit of pre-launch marketing, ...  \n",
       "7  Google has revealed its latest Titan security ...  \n",
       "8  As you are likely already aware, Epic Games is...  \n",
       "9  Before Google moves to bring its human-soundin...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:\\\\Users\\\\Neon\\\\Desktop\\\\working\\\\Incampus 1\\\\Data Collection\\\\Project\\\\news_api.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deepak Dwivedi\n",
    "\n",
    "Question 2: Naukri scrape for hyderabad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing require package\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import math as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## intitaing the chrome driver\n",
    "\n",
    "locationOfWebdriver = \"C:\\\\Users\\\\Neon\\\\Desktop\\\\working\\\\Incampus 1\\\\Data Collection\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(locationOfWebdriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating number of page for loop\n",
    "n=1093\n",
    "page=mt.ceil(n/50)\n",
    "\n",
    "print(page)\n",
    "\n",
    "# Creating a data frame skeleton\n",
    "\n",
    "data=pd.DataFrame(columns=['Company','Skill'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pulling required data  and creating data frame\n",
    "skill_data=[]\n",
    "company=[]\n",
    "\n",
    "driver.get(\"https://www.naukri.com/information-technology-jobs-in-hyderabad-secunderabad\")\n",
    "\n",
    "for i in range(0,page):\n",
    "    time.sleep(5) ## to refresh the page properly\n",
    "    \n",
    "    search=driver.find_elements_by_xpath(\"\"\"//span[@class= \"content\"]\"\"\")## getting content for job posting\n",
    "    if i==0:\n",
    "        for j in range(0,len(search)):# to iterate as many content (in short no of jobs in each page)\n",
    "            com=search[j].find_element_by_xpath(\"\"\".//span[@class= \"org\"]\"\"\")## organisation name using relative path\n",
    "            company.append(com.text)## extra step for satisfaction\n",
    "            com=pd.Series(com.text,index=['Company'])# creating series will be used to append in data frame\n",
    "            try: # for error handeling\n",
    "                sk=search[j].find_element_by_xpath(\"\"\".//span[@class=\"skill\"]\"\"\") ## skill name using relative path   \n",
    "                skill_data.append(sk.text)\n",
    "                sk=pd.Series(sk.text,index=['Skill'])# creating series will be used to append in data frame\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "            data=data.append(pd.concat([com,sk]),ignore_index=True)# appending in dataframe for each iteration\n",
    "        next=driver.find_elements_by_class_name(\"grayBtn\")# SInce first page has only next button so choosing one\n",
    "        next[0].click()#clickig on first\n",
    "    elif i <= 20: # iteration before the last page, because in last page we dont need to \n",
    "        for j in range(0,len(search)):\n",
    "            com=search[j].find_element_by_xpath(\"\"\".//span[@class= \"org\"]\"\"\")\n",
    "            company.append(com.text)\n",
    "            com=pd.Series(com.text,index=['Company'])\n",
    "            try:\n",
    "                sk=search[j].find_element_by_xpath(\"\"\".//span[@class=\"skill\"]\"\"\")    \n",
    "                skill_data.append(sk.text)\n",
    "                sk=pd.Series(sk.text,index=['Skill'])\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "            data=data.append(pd.concat([com,sk]),ignore_index=True)#Appending dataframe\n",
    "        next=driver.find_elements_by_class_name(\"grayBtn\")\n",
    "        next[1].click()# selcting second button nothing for moving ahead\n",
    "    else:\n",
    "        for j in range(0,len(search)):\n",
    "            com=search[j].find_element_by_xpath(\"\"\".//span[@class= \"org\"]\"\"\")\n",
    "            com=pd.Series(com.text,index=['Company'])\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                sk=search[j].find_element_by_xpath(\"\"\".//span[@class=\"skill\"]\"\"\")    \n",
    "                skill_data.append(sk.text)\n",
    "                sk=pd.Series(sk.text,index=['Skill'])\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "            data=data.append(pd.concat([com,sk]),ignore_index=True)#Appending dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## getting all data\n",
    "data.to_csv(\"C:\\\\Users\\\\Neon\\\\Desktop\\\\working\\\\Incampus 1\\\\Data Collection\\\\Project\\\\Naukri_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##getting summary level data\n",
    "\n",
    "data2=data.groupby(\"Company\").count()\n",
    "data2.dtypes\n",
    "data2=data2.drop([\"com_count\"],axis=1)\n",
    "data2.columns=[\"# of posting\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##getting skill in row level\n",
    "\n",
    "data3[\"Skill\"] = data.groupby('Company')['Skill'].apply('#####'.join)\n",
    "print(type(data3))\n",
    "data3.head()\n",
    "\n",
    "skill_data=pd.DataFrame(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merging data Frame\n",
    "\n",
    "data_require=pd.merge(data2,skill_data,on=\"Company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_require.to_csv(\"C:\\\\Users\\\\Neon\\\\Desktop\\\\working\\\\Incampus 1\\\\Data Collection\\\\Project\\\\hyderabad.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deepak Dwivedi\n",
    "Question1 : Indeed scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing require packages \n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## intitaing the chrome driver\n",
    "\n",
    "locationOfWebdriver = \"C:\\\\Users\\\\Neon\\\\Desktop\\\\working\\\\Incampus 1\\\\Data Collection\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(locationOfWebdriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating list for title location and summary for all pages\n",
    "\n",
    "title=[]\n",
    "loc=[]\n",
    "summary=[]\n",
    "for i in range(0,99): ## iterating for all pages(100)\n",
    "    driver.get(\"https://www.indeed.com/jobs?q=Product+Manager&l=San+Francisco%2C+CA&start=\"+str(i)+\"0\")\n",
    "    time.sleep(5)\n",
    "    all_title=driver.find_elements_by_xpath('//div[@class = \"title\"]//a')#getting to anchor tag\n",
    "    all_loc=driver.find_elements_by_xpath('//span[@class=\"location accessible-contrast-color-location\"]')\n",
    "    all_summ=driver.find_elements_by_xpath('//div[@class=\"summary\"]')\n",
    "    for j in range(0,len(all_title)):\n",
    "        title.append(all_title[j].get_attribute(\"title\"))#picking title \n",
    "        loc.append(all_loc[j].text)# location \n",
    "        summary.append(all_summ[j].text) # summary\n",
    "   \n",
    "    \n",
    "## Creating data Frame\n",
    "\n",
    "PMscraping=pd.DataFrame(zip(title,loc,summary),columns=[\"Job Title\", \"Location\",\"Description\"])\n",
    "\n",
    "## exporting to CSV\n",
    "\n",
    "PMscraping.to_csv(\"C:\\\\Users\\\\Neon\\\\Desktop\\\\working\\\\Incampus 1\\\\Data Collection\\\\Project\\\\PMscraping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
